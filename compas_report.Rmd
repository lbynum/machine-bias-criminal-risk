---
title: "Assessing and Mitigating Algorithmic Bias in Criminal Risk Scores"
author: "Lucius Bynum, Preethi Seshadri"
date: '19 November 2017'
output:
  html_document:
    theme: journal
    highlight: tango
    df_print: paged
# output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, cache=TRUE)
options(digits=4)

library(dplyr)
library(GGally)
library(ggplot2)
library(e1071)
```

# Introduction

# Problem Statement

# Data Description

ProPublica collected data on **TODO**

In the data set we look at, they considered only people who either "recidivated within two years of their crime or "recidivated in tow years, or had at least tow years outside of a correctional facility."

### Data Dictionary

ProPublica did not provide a data dictionary explaining their variables. Through some manual exploration, we came up with the following descriptions for our best guess at what each variable measures.

<!-- -id: unque identifier for each individual -->
<!-- -name: first and last name -->
<!-- -first: first name -->
<!-- -last: last name -->
<!-- -compas_screening_dat: date on which decile_score was given -->
<!-- -sex: sex (male or female) -->
<!-- -dob: date of birth -->
<!-- -age: age in years -->
<!-- -age_cat: age category (less than 25, 25-45, greater than 45) -->
<!-- -race: race (African-American, Asian, Caucasian, Hispanic, Native American, Other) -->
<!-- -juv_fel_count: juvenile felony count -->
<!-- -decile_score: COMPAS Risk of Recidivism score from 1 to 10 -->
<!-- -juv_misd_count: juvenile misdemeanor count -->
<!-- -juv_other_count: juvenile other offenses count -->
<!-- -priors_count: prior offenses count -->
<!-- -days_b_screening_arrest: number of days between COMPAS screening and arrest **TODO**: negative values? -->
<!-- -c_jail_in: jail entry date for original crime -->
<!-- -c_jail_out: jail exit date for original crime -->
<!-- -c_case_number: case number for original crime -->
<!-- -c_offense_date: offense date of original crime -->
<!-- -c_arrest_date: arrest date for original crime -->
<!-- -c_days_from_compas: days between COMPAS screening and original crime offense date -->
<!-- -c_charge_degree: charge degree of original crime -->
<!-- -c_charge_desc: description of charge for original crime -->
<!-- -is_recid: binary indicator of recidivation (1=individual recidivated, 0=individual did not recidivate) -->
<!-- -r_case_number: case number of follow-up crime -->
<!-- -r_charge_degree: charge degree of follow-up crime -->
<!-- -r_days_from_arrest: number of days between follow-up crime and arrest date **TODO**: why negative value here? -->
<!-- -r_offense_date: date of follow-up crime -->
<!-- -r_charge_desc: description of charge for follow-up crime -->
<!-- -r_jail_in: jail entry date for follow-up crime -->
<!-- -r_jail_out: jail exit date for follow-up crime -->
<!-- -violent_recid: values are all `NA`. This column is ignored. -->
<!-- -is_voilent_recid: binary indicator of violent follow-up crime (1=follow-up crime was violent, 0=follow-up crime was non-violent) -->
<!-- -vr_case_number: case number for violent follow-up crime -->
<!-- -vr_charge_degree: charge degree for violent follow-up crime -->
<!-- -vr_offense_date: date of offense for violent follow-up crime -->
<!-- -vr_charge_desc: description of charge for violent follow-up crime -->
<!-- -type_of_assessment: the type of COMPAS score given for decile_score (here all values are Risk of Recidivism) -->
<!-- -decile_score.1: repeat column of decile_score -->
<!-- -score_text: ProPublica-defined category of decile_score (High: 8-10, Medium: 5-7, Low: 1-4) -->
<!-- -screening_date: repeat column of compas_screening_date  -->
<!-- -v_type_of_assessment: the type of COMPAS score given for v_decile_score (here all values are Risk_of_Violence) -->
<!-- -v_decile_score: COMPAS Risk of Violence score from 1 to 10 -->
<!-- -v_score_text: ProPublica-defined category of v_decile_score (High: 8-10, Medium: 5-7, Low: 1-4) -->
<!-- -v_screening_date: date on which v_decile_score was given -->
<!-- -in_custody: date on which individual was brought into custody -->
<!-- -out_custody: date on which individual was released from custody -->
<!-- -priors_count.1: repeat column of priors_count -->
<!-- -start: **TODO** -->
<!-- -end: **TODO** -->
<!-- -event: **TODO** -->

| Variable| Description|
|--------------------------|----------------------------------------------------------------------------------------------------------------|
| id                      | unque identifier for each individual|
| name                    | first and last name|
| first                   | first name|
| last                    | last name|
| compas_screening_dat    | date on which decile_score was given|
| sex                     | sex (male or female)|
| dob                     | date of birth|
| age                     | age in years|
| age_cat                 | age category (less than 25, 25-45, greater than 45)|
| race                    | race (African-American, Asian, Caucasian, Hispanic, Native American, Other)|
| juv_fel_count           | juvenile felony count|
| decile_score            | COMPAS Risk of Recidivism score from 1 to 10|
| juv_misd_count          | juvenile misdemeanor count|
| juv_other_count         | juvenile other offenses count|
| priors_count            | prior offenses count|
| days_b_screening_arrest | number of days between COMPAS screening and arrest **TODO** negative values?|
| c_jail_in               | jail entry date for original crime|
| c_jail_out              | jail exit date for original crime|
| c_case_number           | case number for original crime|
| c_offense_date          | offense date of original crime|
| c_arrest_date           | arrest date for original crime|
| c_days_from_compas      | days between COMPAS screening and original crime offense date|
| c_charge_degree         | charge degree of original crime|
| c_charge_desc           | description of charge for original crime|
| is_recid                | binary indicator of recidivation (1=individual recidivated, 0=individual did not recidivate)|
| r_case_number           | case number of follow-up crime|
| r_charge_degree         | charge degree of follow-up crime|
| r_days_from_arrest      | number of days between follow-up crime and arrest date **TODO** why negative value here?|
| r_offense_date          | date of follow-up crime|
| r_charge_desc           | description of charge for follow-up crime|
| r_jail_in               | jail entry date for follow-up crime|
| r_jail_out              | jail exit date for follow-up crime|
| violent_recid           | values are all `NA`. This column is ignored.|
| is_voilent_recid        | binary indicator of violent follow-up crime (1=follow-up crime was violent, 0=follow-up crime was non-violent) |
| vr_case_number          | case number for violent follow-up crime|
| vr_charge_degree        | charge degree for violent follow-up crime|
| vr_offense_date         | date of offense for violent follow-up crime|
| vr_charge_desc          | description of charge for violent follow-up crime|
| type_of_assessment      | the type of COMPAS score given for decile_score (here all values are Risk of Recidivism)|
| decile_score.1          | repeat column of decile_score|
| score_text              | ProPublica-defined category of decile_score (High=8-10, Medium=5-7, Low=1-4)|
| screening_date          | repeat column of compas_screening_date|
| v_type_of_assessment    | the type of COMPAS score given for v_decile_score (here all values are Risk_of_Violence)|
| v_decile_score          | COMPAS Risk of Violence score from 1 to 10|
| v_score_text            | ProPublica-defined category of v_decile_score (High=8-10, Medium=5-7, Low=1-4)|
| v_screening_date        | date on which v_decile_score was given|
| in_custody              | date on which individual was brought into custody|
| out_custody             | date on which individual was released from custody|
| priors_count.1          | repeat column of priors_count|
| start                   | **TODO**|
| end                     | **TODO**|
| event                   | **TODO**|

ProPublica obtained this data with the goal of analyzing Northpointe Inc.’s commercial recidivism modeling tool -- COMPAS. Aggregating data from public records, they collected data on 18,610 individuals who received COMPAS scores from 2013 to 2014, including demographic information, public criminal records, and incarceration records.

*How are COMPAS scores used?*

ProPublica describes that at least in Broward County, they “primarily [use] the score to determine whether to release or detain a defendant before his or her trial.” 11,757 of the individuals in the database had their COMPAS scores used to assess whether or not they should be released before their trial.

*What are COMPAS scores?*

There are three types of COMPAS score. Each measures a type of ‘risk’ associated with a criminal re-offending in some way on a scale of 1 (low) to 10 (high). As ProPublica describes, these include 

- *Risk of Recidivism*: ProPublica defines this as the person in question committing a “criminal offense that [results] in a jail booking and [takes] place after the crime for which the person was COMPAS scored.”  Northpointe hopes to use this score to predict “a new misdemeanor or felony offense within two years of the COMPAS administration date.”
- *Risk of Violence*: They use the FBI definition of violent crime: 
    
    *In the FBI’s Uniform Crime Reporting (UCR) Program, violent crime is composed of four offenses: murder and nonnegligent manslaughter, forcible rape, robbery, and aggravated assault. Violent crimes are defined in the UCR Program as those offenses which involve force or threat of force. - ucr.fbi.gov*

- *Risk of Failure to Appear*: As evidenced by the name, this describes a failure to appear at the court hearing. 


# Exploratory Data Analysis

We first load in the data provided by ProPublica, and take a quick look at summaries of each variable to get a sense of distribution and potential outliers. We'll look at half of the columns at a time to keep things manageable. 

```{r}
compas_data <- read.csv('compas-scores-two-years.csv')
summary(compas_data[1:(ncol(compas_data)/2)])
```

Here we notice there may be large outliers in many of the crime count variables, such as `juv_fel_count`, `juv_misd_count`, `juv_other_count`, and `priors_count`. We expect these are simply accurate observations corresponding to individuals with high numbers of prior offenses. Thus we will not remove these individuals from the data but will be aware of them as potential influential points when we later fit any models. We also note that the values for the 'days_from' variables are quite variable which may be relevant if we use those variables in later analysis. Looking at the second half of the columns, we have:

```{r}
summary(compas_data[(ncol(compas_data)/2 + 1):ncol(compas_data)])
```

With the second half we have similar characteristics as before. We remove the `violent_recid` column given that all values are `NA` (as mentioned in the data dictionary). Apart from that column, we make no other changes.

## ProPublica's Bias-Assessment Model

This section is meant to recreate ProPublica's logistic regression model for assessing bias in COMPAS scores. The code and explanations in this section are adapted from their [published methodlogy and code](https://github.com/propublica/compas-analysis). 

Using the same data set, they first filter rows based on the following criteria.

1. consider only individuals with a COMPAS score
2. assure the COMPAS score corresponds to the correct crime i.e. the score was given within 30 days of the arrest
3. do not include ordinary traffic offenses

Next we use their code to perform this filtering:

```{r}
# code from https://github.com/propublica/compas-analysis
df <- compas_data %>%
  select(age, c_charge_degree, race, age_cat, score_text, sex, priors_count, days_b_screening_arrest, decile_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>% 
  filter(days_b_screening_arrest <= 30) %>%
  filter(days_b_screening_arrest >= -30) %>%
  filter(is_recid != -1) %>%
  filter(c_charge_degree != "O") %>%
  filter(score_text != 'N/A')
nrow(df)
```

In order to use this data to assess racial bias in scoring, the ProPublica analysts first create several factor variables from the existing columns.

```{r}
# code from https://github.com/propublica/compas-analysis
df <- mutate(df, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))
```

For the `age_factor` they make "25 - 45" the reference level, for `race_factor` "Caucasian" is the reference level, for `gender_factor` "Male" is the reference level. 

Next they fit a logistic regression model to predict `score_factor` from the other variables.

```{r}
# code from https://github.com/propublica/compas-analysis
pp_model <- glm(
  score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, 
  family="binomial", 
  data=df
)
summary(pp_model)
```

#### Calculating Relative Risk by Demographic

Next they compute how much more likely different demographic groups are to receive a higher score than others. The logistic regression model allows us to measure this difference after correcting for the other variables included in the model. The quantity ProPublica uses to compare black defendants to white defendants (or men to women, old defendants to young defendants, etc.) is called **relative risk**. ProPublica does not explain where this quantity comes from in their analysis, so we'll provide some quick background on logistic regression to justify the calculation. The following explanation is inspired by USC professor Sandy Eckel's slides [here](http://www-hsc.usc.edu/~eckel/biostat2/notes/notes14.pdf).

Logistic regression models a linear relationship between the log odds ratio for the probability of interest and the given predictor variables. An odds ratio measures the odds of success

$$\texttt{odds ratio} = \frac{\texttt{probability of success}}{\texttt{probability of failure}} = \frac{P}{1 - P}$$

where $P$ is the probability of success. The log odds ratio is simply the log of this quantity. Thus the logistic regression model for observation $x_i$ is

$$\log(\frac{P_{x_i}}{1-P_{x_i}}) = \beta_0 + \beta_1x_{i1} + \ldots + \beta_px_{ip}$$

where the probability of success for $x_i$ is $P_{x_i}$, and we have $p$ predictors with corresponding coeffecients $\beta_j$ and observed values $x_{ij}$ for $j=1,\ldots,p$. Given that our model here uses categorical predictors (factors), the coeffecients we estimate give us the **change in log odds** for the corresponding variable. Thus if we let $P_{x_i}$ be the probability that individual $x_i$ gets a high COMPAS score, then with coefficent $\beta_1$ for `gender_factor`, we would have

$$\beta_0: \text{the log odds of getting a high COMPAS score for men}$$
$$\beta_1: \text{the difference in log odds of getting a high COMPAS score for women compared to men}$$

The important observation here is that because men are the reference level for the `gender_factor` categorical variable, $\beta_1$ measures a **difference** relative to men. Thus if we want to answer the question, "How much more likely are women to get a high COMPAS score than men?" we'll want to use

$$\beta_0 + \beta_1: \text{the log odds of getting a high COMPAS score for women}$$

to get the comparison. One other observation will also be helpful to calculate relative risk. We solve for $P_{x_i}$ as follows.

$$\log(\frac{P}{1 - P}) = x \rightarrow P = \frac{e^x}{1 + e^x} = \texttt{sigmoid}(x)$$

Thus we calculate relative risk for the categorical variable corresponding to $\beta_1$ as:

$$\texttt{relative risk} = \frac{P_1}{P_2} = \frac{\texttt{sigmoid}(\beta_0 + \beta_1)}{\texttt{sigmoid}(\beta_0)}$$

ProPublica computes relative risk to compare blacks to whites, men to women, and people under 25 to middle-aged people in terms of COMPAS scores. They get the following results.

```{r}
# code adapted from https://github.com/propublica/compas-analysis
model_intercept <- coef(pp_model)['(Intercept)']
black_coef <- coef(pp_model)['race_factorAfrican-American']
(relative_risk_black_v_white <- sigmoid(model_intercept + black_coef) / sigmoid(model_intercept))
```

As ProPublica states, this shows us that "Black defendants are 45% more likely than white defendants to receive a higher [COMPAS] score correcting for the seriousness of their crime, previous arrests, and future criminal behavior." Similarly, women are 19.4\% more likely than men and people under 25 are 2.5 times as likely as middle aged people to get a higher score:

```{r}
# code adapted from https://github.com/propublica/compas-analysis
woman_coef <- coef(pp_model)['gender_factorFemale']
(relative_risk_woman_v_man <- sigmoid(model_intercept + woman_coef) / sigmoid(model_intercept))
```

```{r}
# code adapted from https://github.com/propublica/compas-analysis
age_coef <- coef(pp_model)['age_factorLess than 25']
(relative_risk_young_v_middleage <- sigmoid(model_intercept + age_coef) / sigmoid(model_intercept))
```

## Our Logistic Regression Model

### Variable Selection

**TODO**: talk about excluding race but including sex

```{r}
vars <- compas_data %>% 
  select(sex, age, ends_with('count'), c_charge_degree, is_recid)

# ggplot(vars, aes(x = race, y = decile_score)) + geom_boxplot()
```

### Training the Model

```{r}
regmod <- glm(is_recid ~., family = binomial(link='logit'), data = vars)
summary(regmod)
fitted_values <- fitted(regmod)
```

### Parameter Tuning

# Using ProPublica's Bias-Assessment Model on Our Model

### Big TODO: we should nest this bias calculation inside cross-validation to make sure we're assessing our bias on future data (also interesting to look at the bias on the training data, however)

We can see if our model suffers from the same bias using ProPublica's bias-assessment methodology. We'll set up the same model they did, but this time instead of predicting `score_factor` for the COMPAS score, we'll create a `score_factor` variable *from our model's predictions*.

First we create a new data frame with `new_score` as our risk or recidivism score (the fitted values from the model we trained above). We also create a `new_score_text` variable that defines score categories in the same way, but this time based on a 0-1 scale instead of a 0-10 scale. Recall ProPublica defined this category as: High=8-10, Medium=5-7, Low=1-4. Thus we'll do: High=0.75-1.0, Medium=0.45-0.75, Low=0-0.45.

```{r}
our_scores <- compas_data %>%
  mutate(new_score = fitted_values) %>%
  mutate(
    new_score_text = as.factor(
      ifelse(new_score >= 0.75, 'High',
             ifelse(new_score >= 0.45, 'Medium', 'Low')))
  )
```

Now we train the same bias-assessment model, this time using `new_score` instead of `decile_score` and `new_score_text` instead of `score_text`.

```{r}
new_df <- our_scores %>%
  select(age, c_charge_degree, race, age_cat, new_score_text, sex, priors_count, days_b_screening_arrest, new_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>% 
  filter(days_b_screening_arrest <= 30) %>%
  filter(days_b_screening_arrest >= -30) %>%
  filter(is_recid != -1) %>%
  filter(c_charge_degree != "O") %>%
  filter(new_score_text != 'N/A') %>%
  mutate(crime_factor = factor(c_charge_degree)) %>%
  mutate(age_factor = as.factor(age_cat)) %>%
  within(age_factor <- relevel(age_factor, ref = 1)) %>%
  mutate(race_factor = factor(race)) %>%
  within(race_factor <- relevel(race_factor, ref = 3)) %>%
  mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
  within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
  mutate(score_factor = factor(new_score_text != "Low", labels = c("LowScore","HighScore")))
```

```{r}
new_pp_model <- glm(
  score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, 
  family="binomial", 
  data=new_df
)
```

## Bias against black vs. white people

```{r}
model_intercept <- coef(new_pp_model)['(Intercept)']
black_coef <- coef(new_pp_model)['race_factorAfrican-American']
(relative_risk_black_v_white <- sigmoid(model_intercept + black_coef) / sigmoid(model_intercept))
```

## Bias against women vs. men

```{r}
woman_coef <- coef(new_pp_model)['gender_factorFemale']
(relative_risk_woman_v_man <- sigmoid(model_intercept + woman_coef) / sigmoid(model_intercept))
```

## Bias against under 25 vs. middle-aged

```{r}
age_coef <- coef(new_pp_model)['age_factorLess than 25']
(relative_risk_young_v_middleage <- sigmoid(model_intercept + age_coef) / sigmoid(model_intercept))
```

# Mitigating Bias

Much work has been done exploring techniques that allow for fair modeling and prediction in the presence of biased data or algorithms, and this area of study is rich and actively researched. Researchers Faisal Kamiran and Toon Calders, in their 2011 paper 
"Data preprocessing techniques for classification without discrimination," describe several techniques that *alter the given dataset* to (ideally) eliminate the source of the bias. Several other techniques deal with modifying classification/regression algorithms themselves to make fairer predictions. Here, the general idea is to instead clean the bias out of the data, after which normal classification methods can be used. Before we discuss the techniques we employ for dealing with bias, we first introduce a few key concepts.

#### Algorithmic Bias

[This Wikipedia article](https://en.wikipedia.org/wiki/Algorithmic_bias) provides a nice overview of different types of Algorithmic bias and several examples. Bias can manifest in an algorithm in various ways and can have multiple causes. The Wikipedia article linked above uses to term "pre-existing bias" to loosely describe a type of bias resulting from baking social or institutional biases into algorithms. Here the source of bias is not necessarily the algorithm but instead the values encoded into it by its creators or by the data it sees. For our brief foray into algorithmic bias here, we focus on this type of bias under the assumption that some underlying bias does exist in the United States criminal justice system and thus in our data set. 

#### Protected/Sensitive Attributes

The term *protected attribute* or *sensitive attribute* typically refers to a discriptor of an individual upon which it is illegal to discriminate under the Fair Work Act. These include characteristics such as sexual orientation, gender identity, and race. 

#### Combatting Algorithmic Bias

Broadly speaking, research in algorithmic bias works to (1) identify where and in what way bias is present (ProPublica's analysis is one such example), (2) come up with ways to constrain models to enforce fair predictions, or (3) alter the underlying data to minimize bias in the data itself. For our recidivism predictions, we'll employ some techniques from category 3 -- altering the underlying data -- based on Kamiran and Calders' work.

## Mitigating Bias Through Data Preprocessing

Kamiran and Calders outline four types of techniques for preprocessing to mitigate bias, described at length in their paper:

1. **Suppression:** Given a sensitive attribute S, find and remove S and the other features most correlated with S.
2. **Massaging the dataset:** Given a sensitive attribute S, swap the labels of some observations with differing values for S to decrease discrimination while maintaining the same overall class distribution. 
3. **Reweighing:** Up- or down-weight observations in the training data based on whether they have an under- or over-represented combinations of sensitive attribute S and the response variable.  
4. **Sampling:** Calculate sample sizes for combinations of sensitive attribute S and the response that would, as Kamiran and Calders put it, "make the dataset descrimination-free." Then under- or over-sample observations accordingly to create a data set with those proportions.  

Here we focus on Sampling. 

### Applying Sampling to the COMPAS Data

To narrow the scope of this report, we focus on *racial bias* against black people as compared to white people, and we implement sampling to combat this type of bias only. If our example words for racial bias, we could theoretically extend it to each of the protected attributes in the data.

First we need to establish what the class imbalance is between blacks and whites in our data. We look for an imbalance in terms of recidivism rates, since recidivism is what our model predicts.

```{r}
(recid_rates <- compas_data %>%
  select(race, is_recid) %>%
  group_by(race) %>%
  summarize(yes_recid = sum(is_recid == 1), no_recid = sum(is_recid == 0)))
```

Focusing on African-Americans and Caucasians and plotting this table as a bar chart, we have:

```{r}
compas_data %>%
  filter(race %in% c('African-American', 'Caucasian')) %>%
  ggplot(aes(x = is_recid, color = race)) + 
    geom_histogram(bins = 2) +
    facet_wrap(~ race)
```

Kamiran and Calders define four types of observation with respect to a protected attribute and a class label. Their definitions are as follows:

- Deprived community with positive class labels (DP)
- Deprived community with negative class labels (DN)
- Favored community with positive class labels (FP)
- Favored community with negative class labels (FN)

In our case these correspond to:

- DP: African-Americans with `is_recid` = 1
- DN: African-Americans with `is_recid` = 0
- FP: Caucasians with `is_recid` = 1
- FN: Caucasians with `is_recid` = 0

```{r}
DP_data <- compas_data %>%
  filter(race == 'African-American' & is_recid == 1)
DN_data <- compas_data %>%
  filter(race == 'African-American' & is_recid == 0)
FP_data <- compas_data %>%
  filter(race == 'Caucasian' & is_recid == 1)
FN_data <- compas_data %>%
  filter(race == 'Caucasian' & is_recid == 0)
```

The process for sampling to get a non-discrimatory sample is as follows.

#### 1. Compute the expected size for each group if the data were non-discriminatory
    
Here we have the following counts for each group:

```{r}
recid_rates %>% filter(race %in% c('African-American', 'Caucasian'))
```

Thus we'll say the expected number of observations for each group is the mean of the four counts:

```{r}
(expected_num_observations <- mean(c(nrow(DP_data), nrow(DN_data), nrow(FP_data), nrow(FN_data))))
```

#### 2. Under- and Over-sample to eliminate descrimination

We want equal representation for each group (DP, DN, FP, and FN). This means getting `r expected_num_observations` observations of each. We under- or over-sample accordingly. Kamiran and Calders describe two methods for performing this sampling: *Preferential Sampling*, which is more likely to sample boundary observations (with a high probability of being in either class); and *Uniform Sampling*, which samples all points with equal probability. Here we perform Uniform Sampling with replacement. 

```{r}
set.seed(123)
# sample indices
DP_sampled_indices <- sample(1:nrow(DP_data), size = expected_num_observations, replace = TRUE)
DN_sampled_indices <- sample(1:nrow(DN_data), size = expected_num_observations, replace = TRUE)
FP_sampled_indices <- sample(1:nrow(FP_data), size = expected_num_observations, replace = TRUE)
FN_sampled_indices <- sample(1:nrow(FN_data), size = expected_num_observations, replace = TRUE)
```

And we build a new data set from the sampled observations.

```{r}
# build a new data set
sampled_compas_data <- rbind(
  DP_data[DP_sampled_indices,],
  DN_data[DN_sampled_indices,],
  FP_data[FP_sampled_indices,],
  FN_data[FN_sampled_indices,]
)
```

We double check visually that our sampling successfully leveled the classes for each partition.

```{r}
sampled_compas_data %>%
  filter(race %in% c('African-American', 'Caucasian')) %>%
  ggplot(aes(x = is_recid, color = race)) + 
    geom_histogram(bins = 2) +
    facet_wrap(~ race)
```

With Uniform Sampling applied to our data for the *black/white* protected race attributes, we're ready to train our model again and see (1) if its level of bias has decreased and also see (2) how its predictive accuracy has changed. 

#### 3. Train the model on the sampled data

Here we'll train the model following the same process from above and see how our bias (measured by ProPublica's methodology) compares to the bias before sampling.

```{r}
vars <- sampled_compas_data %>% 
  select(sex, age, ends_with('count'), c_charge_degree, is_recid)
post_sample_regmod <- glm(is_recid ~., family = binomial(link='logit'), data = vars)
summary(post_sample_regmod)
```

```{r}
fitted_values <- fitted(post_sample_regmod)
post_sample_scores <- sampled_compas_data %>%
  mutate(new_score = fitted_values) %>%
  mutate(
    new_score_text = as.factor(
      ifelse(new_score >= 0.75, 'High',
             ifelse(new_score >= 0.45, 'Medium', 'Low'))))
```

```{r}
post_sample_df <- post_sample_scores %>%
  select(age, c_charge_degree, race, age_cat, new_score_text, sex, priors_count, days_b_screening_arrest, new_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>% 
  filter(days_b_screening_arrest <= 30) %>%
  filter(days_b_screening_arrest >= -30) %>%
  filter(is_recid != -1) %>%
  filter(c_charge_degree != "O") %>%
  filter(new_score_text != 'N/A') %>%
  mutate(crime_factor = factor(c_charge_degree)) %>%
  mutate(age_factor = as.factor(age_cat)) %>%
  within(age_factor <- relevel(age_factor, ref = '25 - 45')) %>%
  mutate(race_factor = factor(race)) %>%
  within(race_factor <- relevel(race_factor, ref = 'Caucasian')) %>%
  mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
  within(gender_factor <- relevel(gender_factor, ref = 'Male')) %>%
  mutate(score_factor = factor(new_score_text != "Low", labels = c("LowScore","HighScore")))
```

```{r}
post_sample_pp_model <- glm(
  score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, 
  family="binomial", 
  data=post_sample_df
)
```

```{r}
model_intercept <- coef(post_sample_pp_model)['(Intercept)']
black_coef <- coef(post_sample_pp_model)['race_factorAfrican-American']
(post_sample_relative_risk_black_v_white <- sigmoid(model_intercept + black_coef) / sigmoid(model_intercept))
```

We are able to decrease the bias in relative risk by:

```{r}
1 - post_sample_relative_risk_black_v_white / relative_risk_black_v_white
```










